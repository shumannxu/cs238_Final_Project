import os
import numpy as np
import pandas as pd 
import random 
import ast  # Import the ast module for literal_eval

# number of states to test (Assumption: 1 game = 100 states)
NUMBER_OF_STATES_TO_TEST = 100 
# number of games to test 
NUMBER_OF_GAMES_TO_PLAY = 10000
# to denote s numerical value for terminal state 
TERMINAL_STATE_VALUE = 10000
# size of state space (+1 to accommodate for terminal state)
STATE_SPACE = 10001
# size of action space 
ACTION_SPACE = 4

class policy_tester():
    def __init__(self, payout_table):
        # payout table is a numpy array, where (s,a) = expected payout of state, action pair
        self.payout_table = payout_table

    """
    policy table: policy pi to test
    states_to_test: numerical states to feed to policy table & test

    ASSUMES ZERO-INDEXING FOR STATES AND ACTIONS 

    returns average score (average EPA-EPB)
    """
    def test_policy(self, policy_table, states_to_test):
        cumulative_score = 0
        # test each numerical state
        for s in states_to_test:
            # generate policy's action given the state
            a = policy_table[s]
            # generate score for this specific (s,a)
            score = self.payout_table[s][a]
            cumulative_score += score 

        expected_payout = cumulative_score / len(states_to_test)
        return expected_payout
        
    """
    Returns the expected payout result of 5 baseline strategies (in the following order):
    (1) Play Random 
    (2) Max of [Always Playing Action 1, Always Playing Action 2, Always Playing Action 3, Always Playing Action 4]
    """
    def test_baseline_policy(self, states_to_test):
        cumulative_scores = np.array([0,0,0,0,0])
        for s in states_to_test:
            # (1) play random 
            random_action = random.randint(0, ACTION_SPACE - 1)
            cumulative_scores[0] += self.payout_table[s, random_action]
            # (2)(3)(4)(5)
            for i in range(ACTION_SPACE):
                a = i  # 0,1,2,3
                cumulative_scores[i+1] = self.payout_table[s, a]

        cumulative_scores = cumulative_scores / len(states_to_test)
        return [cumulative_scores[0], np.max(cumulative_scores[1:])]

# gets the policy table generated by Q learning or MLE
def get_policy_table(policy_table_fp):
    df = pd.read_csv(policy_table_fp, header=None)
    df = df.apply(lambda x: x - 1)
    policy_table = np.array(df)
    return policy_table
    
"""
Returns None if inputfilepath does not exist
Returns numpy array with State, Action, Rewards, Next State columns 
Zero-indexes states, action, new states
"""
def process_data(inputfilepath):
    # check if path exists
    if not os.path.exists(inputfilepath):
        return None     

    # load data into numpy array
    df = pd.read_csv(inputfilepath, sep=";")
    df["State"] = df["State"].apply(ast.literal_eval)
    df["Next_State"] = df["Next_State"].apply(ast.literal_eval)

    # Change states to numerical state 
    for i in range(len(df["State"])):
        state_list, state_list_prime = df["State"][i], df["Next_State"][i]

        # State: convert to numerical state value 
        curDown, toGo, fp = int(state_list[0]), int(state_list[1]), int(state_list[2])
        # handle edge case, where toGo is greater than 25, curDown is 5+, fp is 99+
        toGo = min(25, toGo)
        curDown = min(4, curDown)
        fp = min(99, fp)
        # update 
        df.loc[i, "State"] = 100 * (toGo - 1) + 2500 * (curDown - 1) + (fp - 1)

        # State prime: account for terminal case 
        if state_list_prime[0] == "T":
            df.loc[i, "Next_State"] = TERMINAL_STATE_VALUE
        else:
            curDown, toGo, fp = int(state_list_prime[0]), int(state_list_prime[1]), int(state_list_prime[2])
            # handle edge case, where toGo is greater than 25, curDown is 5+, fp is 99+
            toGo = min(25, toGo)
            curDown = min(4, curDown)
            fp = min(99, fp)
            # update 
            df.loc[i, "Next_State"] = 100 * (toGo - 1) + 2500 * (curDown - 1) + (fp - 1)

    # zero index the data 
    df["State"] -= 1
    df["Next_State"] -= 1
    df["Action"] -= 1

    # convert to numpy for efficiency 
    data = df.to_numpy()

    return data

"""
Plays 10,000 simulated games, with each game having 100 states. 
The states are sampled based on how frequent each state appears in actual games. 
This function plays 4 strategies:
(1) Model Free
(2) Model Based 
(3) Random Action 
(4) Max of Always Same Action 
This function returns average score per game for each of the 4 strategies.
"""
def simulate_games(states_idx, probability_states, expected_value_table, 
                  policy_table_model_free, policy_table_model_based):
    
    avg_model_based, avg_model_free, avg_random, avg_always_same = 0,0,0,0
    wins = [0,0,0,0]

    # Play 10000 games (ModelFree vs ModelBased vs Random vs Max of always playing same action)
    for i in range(NUMBER_OF_GAMES_TO_PLAY):
        # Sample States based on distribution 
        states_to_test = np.random.choice(states_idx, size=NUMBER_OF_STATES_TO_TEST, p=probability_states.flatten())

        # Test (get average score after simulating 100 states)
        TestingInstance = policy_tester(expected_value_table)
        model_free_score = TestingInstance.test_policy(policy_table_model_free, states_to_test)
        model_based_score = TestingInstance.test_policy(policy_table_model_based, states_to_test)
        random_score, always_same_score = TestingInstance.test_baseline_policy(states_to_test)
        
        print(model_free_score, model_based_score, random_score, always_same_score)

        # Update scores 
        avg_model_free += model_free_score
        avg_model_based += model_based_score
        avg_random += random_score
        avg_always_same += always_same_score

        # Update win count 
        wins[np.argmax([model_free_score, model_based_score, random_score, always_same_score])] += 1
    
    return (avg_model_based, avg_model_free, avg_random, avg_always_same, wins)

"""
Prints the average score of EPA-EPB for each state,action in a simulated game. 
Playes 10,000 games, with each game having 100 states sampled based on frequency distribution of states.
"""
def main():

    ###### Step 1: Create Expected Value/Payout Table via MLE ######

    # Create a cum payout table (divide to get EV later)
    cumulative_rewards_table = np.full((STATE_SPACE, ACTION_SPACE), 0) 
    # Count the occurrences of each state (start with the count 4 for laplace)
    visit_count_table = np.full((STATE_SPACE, ACTION_SPACE), 1)
    # count state appearances 
    visit_count_only_states = np.full((STATE_SPACE, 1), 0)

    # Count occurrences of each (s,a) pair for all files (zero-indexed)
    for dir in ["cleaned_2023_data/23_24_", "cleaned_2022_data/22_23_"]:
        for week_num in range(1, 22): 
            inputfilepath = 'data_cleaned/' + dir + 'week_' + str(week_num) + ".csv"

            # get data from each csv file 
            data = process_data(inputfilepath)
            # check if data is not none 
            if data is None:  continue 
            # traverse through each row in the file (s,a,r,s')
            for row_data in data: 
                (s,a,r,s_prime) = row_data
                # R(s,a)
                cumulative_rewards_table[s][a] += r 
                # N(s,a) 
                visit_count_table[s][a] += 1
                # N(s)
                visit_count_only_states[s] += 1
    
    # EV = R(s,a) / N(s,a)  (with laplace)
    expected_value_table = cumulative_rewards_table / visit_count_table

    ###### Step 2: Calculate distribution of state probability, or N(s) / N ######
    probability_states = visit_count_only_states / np.sum(visit_count_only_states)
    states_idx = np.array([num for num in range(0, STATE_SPACE)])

    ###### Step 3: Get Pre-Generated Policy Tables from Model Based & Model Free Learnings ######

    ### (1) Get Q learning PolicyTable ###
    policy_table_fp_model_free = "results/q_learning.csv"
    policy_table_model_free = get_policy_table(policy_table_fp_model_free)
    ### (2) Get Model Based Policy Table ###
    policy_table_fp_model_based = "results/model_based.csv"
    policy_table_model_based = get_policy_table(policy_table_fp_model_based)

    ###### Step 4: Play Games & Test Scores for Results ######

    # simulate 10000 games, where each game has 100 states sampled based on frequency distribution
    (avg_model_based, avg_model_free, avg_random, avg_always_same, wins) = simulate_games(states_idx, probability_states, 
                                                                                          expected_value_table, policy_table_model_free, 
                                                                                          policy_table_model_based)
    # compute the average score for each policies 
    avg_model_free /= NUMBER_OF_GAMES_TO_PLAY
    avg_model_based /= NUMBER_OF_GAMES_TO_PLAY
    avg_random /= NUMBER_OF_GAMES_TO_PLAY
    avg_always_same /= NUMBER_OF_GAMES_TO_PLAY

    # display results 
    print("\nAvg Score of Playing Model Free Action:", avg_model_free[0], "| won", wins[0], "times")
    print("Avg Score of Playing Model Based Action:", avg_model_based[0], "| won", wins[1], "times")
    print("Avg Score of Playing Random Action:", avg_random, "| won", wins[2], "times")
    print("Avg Score of Always Playing Same Action:", avg_always_same, "| won", wins[3], "times\n")


if __name__ == '__main__':
    main()


    

